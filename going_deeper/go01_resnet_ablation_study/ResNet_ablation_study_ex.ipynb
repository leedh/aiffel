{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4453d40",
   "metadata": {},
   "source": [
    "# 프로젝트: 없다면 어떻게 될까? (ResNet Ablation Study) \n",
    "|평가문항|상세기준|\n",
    "|:---|:---|\n",
    "|1. ResNet-34, ResNet-50 모델 구현이 정상적으로 진행되었는가?|블록함수 구현이 제대로 진행되었으며 구현한 모델의 summary가 예상된 형태로 출력되었다.|\n",
    "|2. 구현한 ResNet 모델을 활용하여 Image Classification 모델 훈련이 가능한가?|tensorflow-datasets에서 제공하는 cats_vs_dogs 데이터셋으로 학습 진행 시 loss가 감소하는 것이 확인되었다.|\n",
    "|3. Ablation Study 결과가 바른 포맷으로 제출되었는가?|ResNet-34, ResNet-50 각각 plain모델과 residual모델을 동일한 epoch만큼 학습시켰을 때의 validation accuracy 기준으로 Ablation Study 결과표가 작성되었다.|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95544074",
   "metadata": {},
   "source": [
    "##  0. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3954c2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /opt/conda/lib/python3.9/site-packages (4.4.0)\n",
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.3-py3-none-any.whl (5.0 MB)\n",
      "     |████████████████████████████████| 5.0 MB 6.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (8.0.3)\n",
      "Collecting dm-tree\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (153 kB)\n",
      "     |████████████████████████████████| 153 kB 25.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: wrapt in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (1.12.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (1.21.4)\n",
      "Collecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (1.1.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (0.12.0)\n",
      "Collecting etils[enp,epath,etree]>=0.9.0\n",
      "  Downloading etils-1.5.2-py3-none-any.whl (140 kB)\n",
      "     |████████████████████████████████| 140 kB 59.5 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: promise in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (4.62.3)\n",
      "Collecting array-record\n",
      "  Downloading array_record-0.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "     |████████████████████████████████| 3.0 MB 43.8 MB/s            \n",
      "\u001b[?25hCollecting protobuf>=3.20\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "     |████████████████████████████████| 294 kB 61.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (2.26.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (5.8.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.9/site-packages (from tensorflow_datasets) (1.5.0)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (4.0.1)\n",
      "Requirement already satisfied: importlib_resources in /opt/conda/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (5.4.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (2021.11.1)\n",
      "Requirement already satisfied: zipp in /opt/conda/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow_datasets) (3.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow_datasets) (2.0.8)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from absl-py->tensorflow_datasets) (1.16.0)\n",
      "  Downloading protobuf-3.20.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     |████████████████████████████████| 1.0 MB 68.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /opt/conda/lib/python3.9/site-packages (from tensorflow-metadata->tensorflow_datasets) (1.53.0)\n",
      "Installing collected packages: etils, protobuf, toml, dm-tree, array-record, tensorflow-datasets\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.19.1\n",
      "    Uninstalling protobuf-3.19.1:\n",
      "      Successfully uninstalled protobuf-3.19.1\n",
      "  Attempting uninstall: tensorflow-datasets\n",
      "    Found existing installation: tensorflow-datasets 4.4.0\n",
      "    Uninstalling tensorflow-datasets-4.4.0:\n",
      "      Successfully uninstalled tensorflow-datasets-4.4.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-gpu 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.4 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed array-record-0.5.0 dm-tree-0.1.8 etils-1.5.2 protobuf-3.20.3 tensorflow-datasets-4.9.3 toml-0.10.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade tensorflow_datasets # 4.9.3 최신버전으로 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b260f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "1.21.4\n",
      "4.4.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds # tensorflow 데이터셋\n",
    "\n",
    "print(tf.__version__)\n",
    "print(np.__version__)\n",
    "print(tfds.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "575a0990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensorflow가 활용할 GPU가 장착되어 있는지 확인\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8fdd85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset 786.68 MiB (download: 786.68 MiB, generated: Unknown size, total: 786.68 MiB) to /aiffel/tensorflow_datasets/cats_vs_dogs/4.0.0...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b23d1729d0984e369bbb7cda843a7126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Completed...: 0 url [00:00, ? url/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cb393fc13cb4ac88bbf5b704074fa0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dl Size...: 0 MiB [00:00, ? MiB/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DownloadError",
     "evalue": "Failed to get url https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip. HTTP code: 404.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDownloadError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_85/2619488884.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#tfds.disable_progress_bar()   # 이 주석을 풀면 데이터셋 다운로드과정의 프로그레스바가 나타나지 않습니다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m (ds_train, ds_test), ds_info = tfds.load(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;34m'cats_vs_dogs'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train[:80%]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train[80%:]'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mRuntimeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mif\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mload\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \"\"\"\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must specify a non-empty dataset to load.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_dir, download_config)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m     ):\n\u001b[0;32m--> 439\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mlegacy_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mnew_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0mfollowing\u001b[0m \u001b[0mcalls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1114\u001b[0m       \u001b[0mdataset_info\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     \"\"\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/image_classification/cats_vs_dogs.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     68\u001b[0m         homepage=(\n\u001b[1;32m     69\u001b[0m             \u001b[0;34m\"https://www.microsoft.com/en-us/download/details.aspx?id=54765\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         ),\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0mcitation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_CITATION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m       \u001b[0mresource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[0;34m(map_fn, all_inputs)\u001b[0m\n\u001b[1;32m    765\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpected_url_info\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpected_url_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Filename unknown.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m   \u001b[0mmanual_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanual_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mexpected_url_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmanual_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# File not manually downloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    765\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpected_url_info\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexpected_url_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Filename unknown.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 767\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    768\u001b[0m   \u001b[0mmanual_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanual_dir\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mexpected_url_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmanual_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# File not manually downloaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/promise/promise.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/promise/promise.py\u001b[0m in \u001b[0;36m_target_settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    514\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;31m# type: (bool) -> Any\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settled_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_reason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/promise/promise.py\u001b[0m in \u001b[0;36m_settled_value\u001b[0;34m(self, _raise)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_raise\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0mraise_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                 \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraise_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_traceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/promise/promise.py\u001b[0m in \u001b[0;36mhandle_future_result\u001b[0;34m(future)\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;31m# type: (Any) -> None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mresolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/download/downloader.py\u001b[0m in \u001b[0;36m_sync_download\u001b[0;34m(self, url, destination_path, verify)\u001b[0m\n\u001b[1;32m    214\u001b[0m       \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mto\u001b[0m \u001b[0mverify\u001b[0m \u001b[0mssl\u001b[0m \u001b[0mcertificates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m       \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/download/downloader.py\u001b[0m in \u001b[0;36m_open_with_requests\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m   \"\"\"\n\u001b[1;32m    277\u001b[0m   \u001b[0;31m# Download FTP urls with `urllib`, otherwise use `requests`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m   \u001b[0mopen_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_with_urllib\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ftp'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_open_with_requests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mopen_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/download/downloader.py\u001b[0m in \u001b[0;36m_assert_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m def _open_with_urllib(\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m ) -> Iterator[Tuple[Response, Iterable[bytes]]]:\n",
      "\u001b[0;31mDownloadError\u001b[0m: Failed to get url https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip. HTTP code: 404."
     ]
    }
   ],
   "source": [
    "import urllib3\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "#tfds.disable_progress_bar()   # 이 주석을 풀면 데이터셋 다운로드과정의 프로그레스바가 나타나지 않습니다.\n",
    "\n",
    "(ds_train, ds_test), ds_info = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:80%]', 'train[80%:]'],\n",
    "    as_supervised=True,\n",
    "    shuffle_files=True,\n",
    "    with_info=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e3426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow 데이터셋을 로드 후 feature 정보 확인\n",
    "print(ds_info.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408bfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터의 개수도 확인\n",
    "print(tf.data.experimental.cardinality(ds_train))\n",
    "print(tf.data.experimental.cardinality(ds_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafdf9f3",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd4df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지의 표현이 0과 1 사이로 들어오는 정규화 코드\n",
    "def normalize_and_resize_img(image, label):\n",
    "    \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
    "    image = tf.image.resize(image, (224,224))\n",
    "    return tf.cast(image, tf.float32) / 255., label\n",
    "\n",
    "def apply_normalize_on_dataset(ds, is_test=False, batch_size=16):\n",
    "    ds = ds.map(\n",
    "        normalize_and_resize_img, \n",
    "        num_parallel_calls=1\n",
    "    )\n",
    "    ds = ds.batch(batch_size)\n",
    "    if not is_test:\n",
    "        ds = ds.repeat()\n",
    "        ds = ds.shuffle(200)\n",
    "    ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e386e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = ds_info.features[\"label\"].num_classes\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13303d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e622d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 일부 시각화\n",
    "fig = tfds.show_examples(ds_train, ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740b8710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 일부 시각화\n",
    "fig = tfds.show_examples(ds_test, ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59964b42",
   "metadata": {},
   "source": [
    "## 2. Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df1c23",
   "metadata": {},
   "source": [
    "### building block 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5047cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for building VGG Block\n",
    "\n",
    "def build_resnet_block(input_layer,\n",
    "                    num_cnn=3, \n",
    "                    channel=64,\n",
    "                    block_num=1,\n",
    "                    is_50=False\n",
    "                   ):\n",
    "    # 입력 레이어\n",
    "    x = input_layer\n",
    "    \n",
    "    # conv 레이어\n",
    "    if is_50 == False: # ResNet34\n",
    "        # shortcut\n",
    "        shortcut = keras.layers.Conv2D(\n",
    "            filters=channel,\n",
    "            kernel_size=(1,1)\n",
    "        )(input_layer)\n",
    "        \n",
    "        # 디폴트 3개 CNN 레이어\n",
    "        for cnn_num in range(num_cnn): \n",
    "            x = keras.layers.Conv2D(\n",
    "                    filters=channel,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding='same',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    name=f'block{block_num}_conv{cnn_num}_1'\n",
    "                )(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "            x = keras.layers.Conv2D(\n",
    "                    filters=channel,\n",
    "                    kernel_size=(3,3),\n",
    "                    padding='same',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    name=f'block{block_num}_conv{cnn_num}_2'\n",
    "                )(x)\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "\n",
    "            # f(x) + x\n",
    "            x = keras.layers.Add(name=f'block{block_num}_add_{cnn_num}')([shortcut, x])\n",
    "            x = keras.layers.Activation(\"relu\", name=f'block{block_num}_out_{cnn_num}')(x)\n",
    "    \n",
    "    \n",
    "    else: #ResNet50\n",
    "        #shortcut\n",
    "        shortcut = keras.layers.Conv2D(\n",
    "                filters=channel*4,\n",
    "                kernel_size=(1,1)\n",
    "            )(input_layer)\n",
    "    \n",
    "        # 디폴트 3개 CNN 레이어\n",
    "        for cnn_num in range(num_cnn): \n",
    "\n",
    "            # 1x1 CNN 레이어\n",
    "            x = keras.layers.Conv2D(\n",
    "                        filters=channel,\n",
    "                        kernel_size=(1,1),\n",
    "                        kernel_initializer='he_normal',\n",
    "                        padding='same',\n",
    "                        name=f'block{block_num}_conv{cnn_num}_1x1'\n",
    "                    )(x)\n",
    "            \n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "\n",
    "            # 3x3 CNN 레이어\n",
    "            x = keras.layers.Conv2D(\n",
    "                        filters=channel,\n",
    "                        kernel_size=(3,3),\n",
    "                        kernel_initializer='he_normal',\n",
    "                        padding='same',\n",
    "                        name=f'block{block_num}_conv{cnn_num}_3x3'\n",
    "                    )(x)\n",
    "    \n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            x = keras.layers.Activation('relu')(x)\n",
    "            \n",
    "            \n",
    "            # 1x1 CNN 레이어\n",
    "            x = keras.layers.Conv2D(\n",
    "                filters=channel*4,\n",
    "                kernel_size=(1,1),\n",
    "                activation='relu',\n",
    "                kernel_initializer='he_normal',\n",
    "                padding='same',\n",
    "                name=f'block{block_num}_conv{cnn_num}_1x1_2'\n",
    "            )(x)\n",
    "            \n",
    "            # 마지막 batch normalization\n",
    "            x = keras.layers.BatchNormalization()(x)\n",
    "            \n",
    "            # f(x) + x\n",
    "            x = keras.layers.Add(name=f'block{block_num}_add_{cnn_num}')([shortcut, x])\n",
    "            x = keras.layers.Activation(\"relu\", name=f'block{block_num}_out_{cnn_num}')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fc43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# block 생성 테스트\n",
    "resnet_input_layer = keras.layers.Input(shape=(32,32,3))   # 입력 레이어 생성\n",
    "resnet_block_output = build_resnet_block(resnet_input_layer)    # ResNet 블록 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 블록 1개짜리 model 생성\n",
    "model = keras.Model(inputs=resnet_input_layer, outputs=resnet_block_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2947957d",
   "metadata": {},
   "source": [
    "### ResNet 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NesNet 모델 자체를 생성하는 함수\n",
    "def build_resnet(input_shape=(32,32,3),\n",
    "              num_cnn_list=[3,4,6,3],\n",
    "              channel_list=[64,128,256,512],\n",
    "              num_classes=num_classes,\n",
    "              is_50=False):\n",
    "    \n",
    "    assert len(num_cnn_list) == len(channel_list) #모델을 만들기 전에 config list들이 같은 길이인지 확인합니다.\n",
    "    \n",
    "    input_layer = keras.layers.Input(shape=input_shape)  # input layer를 만들어둡니다.\n",
    "#     output = input_layer\n",
    "    \n",
    "    # 첫 번째 Conv 레이어\n",
    "    x = keras.layers.Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=(7,7),\n",
    "        strides=2\n",
    "    )(input_layer)\n",
    "    \n",
    "    # Max Pooling 레이어\n",
    "    output = keras.layers.MaxPooling2D(\n",
    "        pool_size=(3, 3),\n",
    "        strides=2\n",
    "    )(x)\n",
    "    \n",
    "    # config list들의 길이만큼 반복해서 두 번째 - 다섯 번째 Conv 블록을 생성\n",
    "    for i, (num_cnn, channel) in enumerate(zip(num_cnn_list, channel_list)):\n",
    "        output = build_resnet_block(\n",
    "            output,\n",
    "            num_cnn=num_cnn, \n",
    "            channel=channel,\n",
    "            block_num=i,\n",
    "            is_50=is_50\n",
    "        )\n",
    "    \n",
    "    output = keras.layers.GlobalAveragePooling2D(name=\"avg_pool\")(output)\n",
    "    output = keras.layers.Flatten(name='flatten')(output)\n",
    "    output = keras.layers.Dense(1000, activation='relu', name='fc2')(output)\n",
    "    output = keras.layers.Dense(1, activation='sigmoid', name='predictions')(output)\n",
    "    \n",
    "    \n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=input_layer, \n",
    "        outputs=output\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1908131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_34 = build_resnet(input_shape=(224, 224, 3), is_50=False)\n",
    "resnet_34.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_50 = build_resnet(input_shape=(224, 224, 3), is_50=True)\n",
    "resnet_50.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65399098",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df374f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = apply_normalize_on_dataset(ds_train, batch_size=BATCH_SIZE)\n",
    "ds_test = apply_normalize_on_dataset(ds_test, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e27cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41438401",
   "metadata": {},
   "source": [
    "## 3. Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76014c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_34.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, clipnorm=1.),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_34 = resnet_34.fit(\n",
    "    ds_train,\n",
    "    steps_per_epoch=int(ds_info.splits['train[:80%]'].num_examples/BATCH_SIZE),\n",
    "    validation_steps=int(ds_info.splits['train[80%:]'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6b7b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_50.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=tf.keras.optimizers.SGD(learning_rate=0.01, clipnorm=1.),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "history_50 = resnet_50.fit(\n",
    "    ds_train,\n",
    "    steps_per_epoch=int(ds_info.splits['train[:80%]'].num_examples/BATCH_SIZE),\n",
    "    validation_steps=int(ds_info.splits['train[80%:]'].num_examples/BATCH_SIZE),\n",
    "    epochs=EPOCH,\n",
    "    validation_data=ds_test,\n",
    "    verbose=1,\n",
    "    use_multiprocessing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d065557",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history_16.history['loss'], 'r')\n",
    "plt.plot(history_19.history['loss'], 'b')\n",
    "plt.title('Model training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['vgg_16', 'vgg_19'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1946ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history_16.history['val_accuracy'], 'r')\n",
    "plt.plot(history_19.history['val_accuracy'], 'b')\n",
    "plt.title('Model validation accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['vgg_16', 'vgg_19'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
