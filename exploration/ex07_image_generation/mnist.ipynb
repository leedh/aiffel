{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6993aab5",
   "metadata": {},
   "source": [
    "## 데이터 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb1b085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow-datasets           4.4.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip list | grep tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22106960",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_177/3818330011.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m mnist, info =  tfds.load(\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"mnist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# needs to happen before anything else, since the imports below will try to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# import tensorflow, too.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mtf_compat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_tf_install\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# pylint:disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommunity\u001b[0m  \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madd_data_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/community/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Community dataset API.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuggingface_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmock_builtin_to_use_gfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuggingface_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmock_huggingface_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommunity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuilder_cls_from_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/community/huggingface_wrapper.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_info.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlazy_imports_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnaming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msplits\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msplits_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeature_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/splits.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproto\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mproto_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtfrecords_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/proto/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\"\"\"Public API of the proto package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdataset_info_generated_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdataset_info_pb2\u001b[0m  \u001b[0;31m# pylint: disable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mSplitInfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_info_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplitInfo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_datasets/core/proto/dataset_info_generated_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0m_sym_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_symbol_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv0\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mschema_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow__metadata_dot_proto_dot_v0_dot_schema__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv0\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatistics_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow__metadata_dot_proto_dot_v0_dot_statistics__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_metadata/proto/v0/schema_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0many_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgoogle_dot_protobuf_dot_any__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv0\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpath_pb2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtensorflow__metadata_dot_proto_dot_v0_dot_path__pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/tensorflow_metadata/proto/v0/path_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mcontaining_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   fields=[\n\u001b[0;32m---> 36\u001b[0;31m     _descriptor.FieldDescriptor(\n\u001b[0m\u001b[1;32m     37\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tensorflow.metadata.v0.Path.step'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mnumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpp_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/google/protobuf/descriptor.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, full_name, index, number, type, cpp_type, label, default_value, message_type, enum_type, containing_type, is_extension, extension_scope, options, serialized_options, has_default_value, containing_oneof, json_name, file, create_key)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0mhas_default_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontaining_oneof\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                 file=None, create_key=None):  # pylint: disable=redefined-builtin\n\u001b[0;32m--> 553\u001b[0;31m       \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_CheckCalledFromGeneratedFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mis_extension\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFindExtensionByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Descriptors cannot be created directly.\nIf this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\nIf you cannot immediately regenerate your protos, some other possible workarounds are:\n 1. Downgrade the protobuf package to 3.20.x or lower.\n 2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n\nMore information: https://developers.google.com/protocol-buffers/docs/news/2022-05-06#python-updates"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "mnist, info =  tfds.load(\n",
    "    \"mnist\", split=\"train\", with_info=True\n",
    ")\n",
    "\n",
    "fig = tfds.show_examples(mnist, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b8a5c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf<=3.20.1\n",
      "  Downloading protobuf-3.20.1-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "     |████████████████████████████████| 1.0 MB 4.6 MB/s            \n",
      "\u001b[?25hInstalling collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.25.3\n",
      "    Uninstalling protobuf-4.25.3:\n",
      "      Successfully uninstalled protobuf-4.25.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0.post1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 3.20.1 which is incompatible.\n",
      "tensorflow-metadata 1.5.0 requires absl-py<0.13,>=0.9, but you have absl-py 2.1.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires absl-py~=0.10, but you have absl-py 2.1.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires flatbuffers~=1.12.0, but you have flatbuffers 23.5.26 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires numpy~=1.19.2, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
      "tensorflow-gpu 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 4.0.1 which is incompatible.\u001b[0m\n",
      "Successfully installed protobuf-3.20.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade \"protobuf<=3.20.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2493911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 픽셀 값을 -1~1 사이의 범위로 변경했고, 레이블 정보를 원-핫 인코딩(one-hot encoding)했습니다.\n",
    "# GAN과 cGAN 각각을 실험해 보기 위해 label 정보 사용 유무에 따라 gan_preprocessing()과 cgan_preprocessing() 두 가지 함수를 구성해 놓았습니다.\n",
    "import tensorflow as tf\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def gan_preprocessing(data):\n",
    "    image = data[\"image\"]\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def cgan_preprocessing(data):\n",
    "    image = data[\"image\"]\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    \n",
    "    label = tf.one_hot(data[\"label\"], 10)\n",
    "    return image, label\n",
    "\n",
    "gan_datasets = mnist.map(gan_preprocessing).shuffle(1000).batch(BATCH_SIZE)\n",
    "cgan_datasets = mnist.map(cgan_preprocessing).shuffle(100).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab17b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i,j in cgan_datasets : break\n",
    "\n",
    "# 이미지 i와 라벨 j가 일치하는지 확인해 봅니다.     \n",
    "print(\"Label :\", j[0])\n",
    "print(\"Image Min/Max :\", i.numpy().min(), i.numpy().max())\n",
    "plt.imshow(i.numpy()[0,...,0], plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0245c9d",
   "metadata": {},
   "source": [
    "이번 구현은 Tensorflow2의 Subclassing 방법을 이용하겠습니다. Subclassing 방법은 tensorflow.keras.Model 을 상속받아 클래스를 만들며, 일반적으로 __init__() 메서드 안에서 레이어 구성을 정의하고, 구성된 레이어를 call() 메서드에서 사용해 forward propagation을 진행합니다. 이러한 Subclassing 방법은 PyTorch의 모델 구성 방법과도 매우 유사하므로 이에 익숙해진다면 Pytorch의 모델 구성 방법도 빠르게 습득할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290ec96",
   "metadata": {},
   "source": [
    "### GAN Generator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c0a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Input, Model\n",
    "\n",
    "class GeneratorGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(GeneratorGAN, self).__init__()\n",
    "\n",
    "        # 활성화함수를 'relu'를 사용하고 unit이 128인 Dense Layer를 정의해주세요,.\n",
    "        self.dense_1 = layers.Dense(128, activation='relu')\n",
    "        # 활성화함수를 'relu'를 사용하고 unit이 256인 Dense Layer를 정의해주세요.\n",
    "        self.dense_2 = layers.Dense(256, activation='relu')\n",
    "        # 활성화함수를 'relu'를 사용하고 unit이 512인 Dense Layer를 정의해주세요.\n",
    "        self.dense_3 = layers.Dense(512, activation='relu')\n",
    "        # 활성화함수를 하이퍼볼릭 탄젠트를 사용하고 unit이 256인 Dense Layer를 정의해주세요.\n",
    "        self.dense_4 = layers.Dense(256, activation='tanh')\n",
    "        #self.dense_4 = layers.Dense(28 * 28 * 1, activation='tanh')\n",
    "        # 모양일 (28,28,1)로 변경해주세요.\n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "\n",
    "    def call(self, noise):\n",
    "        out = self.dense_1(noise)\n",
    "        out = self.dense_2(out)\n",
    "        out = self.dense_3(out)\n",
    "        out = self.dense_4(out)\n",
    "        return self.reshape(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45d77d1",
   "metadata": {},
   "source": [
    "### cGAN Generator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3623f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(GeneratorCGAN, self).__init__()\n",
    "        \n",
    "        self.dense_z = layers.Dense(256, activation='relu')\n",
    "        self.dense_y = layers.Dense(256, activation='relu')\n",
    "        self.combined_dense = layers.Dense(512, activation='relu')\n",
    "        self.final_dense = layers.Dense(28 * 28 * 1, activation='tanh')\n",
    "        self.reshape = layers.Reshape((28, 28, 1))\n",
    "\n",
    "    def call(self, noise, label):\n",
    "        # 노이즈에 Dense layer를 적용시킵니다.\n",
    "        noise = self.dense_z(noise)\n",
    "         # 라벨에 Dense layer를 적용시킵니다.\n",
    "        label = self.dense_y(label)\n",
    "        # 노이즈와 라벨을 pair가 되게 합친 다음 combined_dense를 적용시킵니다. (힌트 : https://www.tensorflow.org/api_docs/python/tf/concat)\n",
    "        out = self.combined_dense(tf.concat([noise, label], axis=-1))\n",
    "        # 마지막 Dense Layer를 적용시킵니다.\n",
    "        out = self.final_dense(out)\n",
    "        \n",
    "        return self.reshape(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e5d663",
   "metadata": {},
   "source": [
    "### GAN Discriminator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed2012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorGAN, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        #해당 방식은 반복문을 활용해 layer를 쌓기 때문에 좋은 테크닉중 하나입니다,.\n",
    "        self.blocks = []\n",
    "        for f in [512, 256, 128, 1]:\n",
    "            self.blocks.append(\n",
    "                layers.Dense(f, activation=None if f==1 else \"relu\")\n",
    "            )\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.flatten(x)\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f09517",
   "metadata": {},
   "source": [
    "### cGAN Discriminator 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbb396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maxout(layers.Layer):\n",
    "    def __init__(self, units, pieces):\n",
    "        super(Maxout, self).__init__()\n",
    "        self.dense = layers.Dense(units*pieces, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(.5)    \n",
    "        self.reshape = layers.Reshape((-1, pieces, units))\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.reshape(x)\n",
    "        return tf.math.reduce_max(x, axis=2)\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f39ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscriminatorCGAN(Model):\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorCGAN, self).__init__()\n",
    "        self.flatten = layers.Flatten()\n",
    "        \n",
    "        self.image_block = Maxout(240, 5)\n",
    "        self.label_block = Maxout(50, 5)\n",
    "        self.combine_block = Maxout(240, 4)\n",
    "        \n",
    "        self.dense = layers.Dense(1, activation=None)\n",
    "    \n",
    "    def call(self, image, label):\n",
    "        image = self.flatten(image)\n",
    "        image = self.image_block(image)\n",
    "        label = self.label_block(label)\n",
    "        x = layers.Concatenate()([image, label])\n",
    "        x = self.combine_block(x)\n",
    "        return self.dense(x)\n",
    "    \n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84f891",
   "metadata": {},
   "source": [
    "## 3. 학습 및 테스트하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45d80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers, losses\n",
    "\n",
    "bce = losses.BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return bce(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    return bce(tf.ones_like(real_output), real_output) + bce(tf.zeros_like(fake_output), fake_output)\n",
    "\n",
    "gene_opt = optimizers.Adam(1e-4)\n",
    "disc_opt = optimizers.Adam(1e-4)    \n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485018fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_generator = GeneratorGAN()\n",
    "gan_discriminator = DiscriminatorGAN()\n",
    "\n",
    "@tf.function()\n",
    "def gan_step(real_images):\n",
    "    noise = tf.random.normal([real_images.shape[0], 100])\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator를 이용해 가짜 이미지 생성\n",
    "        fake_images = gan_generator(noise)\n",
    "        # Discriminator를 이용해 진짜 및 가짜이미지를 각각 판별\n",
    "        real_out = gan_discriminator(real_images)\n",
    "        fake_out = gan_discriminator(fake_images)\n",
    "        # 각 손실(loss)을 계산\n",
    "        gene_loss = generator_loss(fake_out)\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "    # gradient 계산\n",
    "    gene_grad = tape.gradient(gene_loss, gan_generator.trainable_variables)\n",
    "    disc_grad = tape.gradient(disc_loss, gan_discriminator.trainable_variables)\n",
    "    # 모델 학습\n",
    "    gene_opt.apply_gradients(zip(gene_grad, gan_generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad, gan_discriminator.trainable_variables))\n",
    "    return gene_loss, disc_loss\n",
    "\n",
    "print(\"✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    for i, images in enumerate(gan_datasets):\n",
    "        gene_loss, disc_loss = gan_step(images)\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[{epoch}/{EPOCHS} EPOCHS, {i+1} ITER] G:{gene_loss}, D:{disc_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157f5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "noise = tf.random.normal([10, 100])\n",
    "\n",
    "output = gan_generator(noise)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bd41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EPOCHS = 500 학습한 가중치 불러오기\n",
    "import os\n",
    "weight_path = os.getenv('HOME')+'/aiffel/conditional_generation/gan/GAN_500'\n",
    "\n",
    "noise = tf.random.normal([10, 100]) \n",
    "\n",
    "gan_generator = GeneratorGAN()\n",
    "gan_generator.load_weights(weight_path)\n",
    "\n",
    "output = gan_generator(noise)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0128a88",
   "metadata": {},
   "source": [
    "### cGAN으로 MNIST 학습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c713f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#위에 있는 gan_step()을 참고해서 cgan_step을 완성해주세요.\n",
    "\n",
    "cgan_generator = GeneratorCGAN()\n",
    "cgan_discriminator = DiscriminatorCGAN()\n",
    "\n",
    "@tf.function()\n",
    "def cgan_step(real_images, labels):\n",
    "    noise = tf.random.normal([real_images.shape[0], 100])\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        fake_images = cgan_generator(noise, labels)\n",
    "        \n",
    "        real_out = cgan_discriminator(real_images, labels)\n",
    "        fake_out = cgan_discriminator(fake_images, labels)\n",
    "        \n",
    "        gene_loss = generator_loss(fake_out)\n",
    "        disc_loss = discriminator_loss(real_out, fake_out)\n",
    "    \n",
    "    gene_grad = tape.gradient(gene_loss, cgan_generator.trainable_variables)\n",
    "    disc_grad = tape.gradient(disc_loss, cgan_discriminator.trainable_variables)\n",
    "    \n",
    "    gene_opt.apply_gradients(zip(gene_grad, cgan_generator.trainable_variables))\n",
    "    disc_opt.apply_gradients(zip(disc_grad, cgan_discriminator.trainable_variables))\n",
    "    return gene_loss, disc_loss\n",
    "\n",
    "\n",
    "EPOCHS = 1\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(cgan_datasets):\n",
    "        gene_loss, disc_loss = cgan_step(images, labels)\n",
    "    \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"[{epoch}/{EPOCHS} EPOCHS, {i} ITER] G:{gene_loss}, D:{disc_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 2 # TODO : 생성할 숫자를 입력해 주세요!!\n",
    "\n",
    "weight_path = os.getenv('HOME')+'/aiffel/conditional_generation/cgan/CGAN_500'\n",
    "\n",
    "noise = tf.random.normal([10, 100])\n",
    "\n",
    "label = tf.one_hot(number, 10)\n",
    "label = tf.expand_dims(label, axis=0)\n",
    "label = tf.repeat(label, 10, axis=0)\n",
    "\n",
    "generator = GeneratorCGAN()\n",
    "generator.load_weights(weight_path)\n",
    "\n",
    "output = generator(noise, label)\n",
    "output = np.squeeze(output.numpy())\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(output[i-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
